---
layout: post
title: "Critical Doing: SIFT for AI"
description: A new method focusing on a SIFT-like framework of moves around AI and improving AI input through well-crafted, evidence-based follow up questions
date: 2024-11-13
image: '/images/521.jpg'
tags: [SIFT, AI]
---
> ...the core question of personal fact-checking as “Is this what people think it is?” There’s actually some deep epistemological insights under that shift, but the simplest way to conceptualize it is being misinformed (either by yourself or others) is not about the relation between something you are looking at and the truth. Being misinformed is usually about bad evidence. The most common pattern is this: without context something looks like good evidence of something. Once you have the context of that evidence it does not.
>
> <cite>Mike Caulfield</cite>

<a href="{{site.baseurl}}/files/Critical Doing_ SIFT for AI-1.pdf" class="button" target="_blank">In Development: SIFT for AI Workshop</a>

I also highly recommend Caulfield's Deep Background: Fact-checks and Context GPT:
<a href="https://chatgpt.com/g/g-684fa334fb0c8191910d50a70baad796-deep-background-fact-checks-and-context" class="button" target="_blank">Deep Background GPT</a>

and his YouTube channel dedicated to SIFT for AI:
<p><iframe src="https://www.youtube.com/embed/xBbP_DophvE" frameborder="0" allowfullscreen></iframe></p>
